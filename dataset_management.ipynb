{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whales identification from their tails - Kaggle contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Manage dataset loading\n",
    "    \n",
    "    :param dataset_path: str, path to the dataset folder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        \n",
    "        # Build and store dataset paths\n",
    "        self.dataset_path_train = dataset_path + \"/train\"\n",
    "        self.dataset_path_test = dataset_path + \"/test\"\n",
    "        \n",
    "        # Build train.csv path\n",
    "        self.dataset_path_train_label = dataset_path + \"/train.csv\"\n",
    "        \n",
    "        # Generate pandas dataframe of whales id <-> file matching\n",
    "        self.dataset_train_label = self.get_train_label()\n",
    "        \n",
    "        ## Get pictures paths\n",
    "#         self.les_im_path_train = self.get_im_path(self.dataset_path_train + \"/*.jpg\")\n",
    "#         self.les_im_path_test = self.get_im_path(self.dataset_path_test + \"/*.jpg\")\n",
    "        \n",
    "    def get_im_path(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Get pictures path under the given folder path\n",
    "        :param dataset_path: str, path to the dataset folder\n",
    "        \n",
    "        :output les_im_path: list of string, .jpg picture paths under the dataset_path folder\n",
    "        \"\"\"\n",
    "        print(\"Getting images path from\", dataset_path)\n",
    "        les_im_path = glob.glob(dataset_path)\n",
    "        les_im_path.sort()\n",
    "        return les_im_path\n",
    "    \n",
    "    def get_train_label(self):\n",
    "        \"\"\"\n",
    "        Load the train dataset annotation using pandas\n",
    "        \n",
    "        :return train_label: pandas dataframe, whales id <-> files matching\n",
    "        \"\"\"\n",
    "        print(\"Loading\", self.dataset_path_train_label)\n",
    "        return pd.read_csv(self.dataset_path_train_label)\n",
    "    \n",
    "    \n",
    "    def create_folder_is_needed(self, folder_path):\n",
    "        \"\"\"\n",
    "        Create a folder if it doesn't alreadt exist\n",
    "        :param folder_path: str\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "    \n",
    "    def split_in_classes_folders(self, root_classes_folder, remove_new_whale=True, remove_old=True, train_dev_ratio=0.2, crop=False, split=True):\n",
    "        \"\"\"\n",
    "        Split the dataset into classes folders\n",
    "        :param root_classes_folder:\n",
    "        :param remove_new_whale: boolean, set to true if not considering new_whale id\n",
    "        \n",
    "        OUTPUT:\n",
    "            pictures sort by whale id into subfolders of the root_classes_folder\n",
    "        \"\"\"\n",
    "        \n",
    "        if remove_old and os.path.isdir(root_classes_folder):\n",
    "            print(\"Removing previous spliting\")\n",
    "            shutil.rmtree(root_classes_folder)\n",
    "        \n",
    "        sub_dataset_train_path = \"/\".join([root_classes_folder, \"train\"])\n",
    "        sub_dataset_dev_path = \"/\".join([root_classes_folder, \"dev\"])\n",
    "        \n",
    "        # Create the folders if needed\n",
    "        self.create_folder_is_needed(root_classes_folder)\n",
    "        self.create_folder_is_needed(sub_dataset_train_path)\n",
    "        self.create_folder_is_needed(sub_dataset_dev_path)\n",
    "        \n",
    "        # If passing new whales, should remove previous folder\n",
    "        if remove_new_whale and os.path.isdir(root_classes_folder):\n",
    "            print(\"Removing new_whale folder\")\n",
    "            train_new_whale = \"/\".join([sub_dataset_train_path, \"new_whale\"])\n",
    "            if os.path.isdir(train_new_whale):\n",
    "                shutil.rmtree(train_new_whale)\n",
    "            dev_new_whale = \"/\".join([sub_dataset_dev_path, \"new_whale\"])\n",
    "            if os.path.isdir(dev_new_whale):\n",
    "                shutil.rmtree(dev_new_whale)\n",
    "        \n",
    "        # Sorting the images\n",
    "        files_number = len(self.dataset_train_label)\n",
    "        if split:\n",
    "            print(\"Sorting and preprocessing\", files_number, \"images into\", root_classes_folder)\n",
    "        else:\n",
    "            print(\"Preprocessing\", files_number, \"images into\", root_classes_folder)\n",
    "        \n",
    "        for index, row in self.dataset_train_label.iterrows():\n",
    "            print(\"#\" + str(index + 1) + \"/\" + str(files_number), end=\"\\r\")\n",
    "            whale_file_name = row['Image']\n",
    "            if split:\n",
    "                whale_id = row['Id']\n",
    "            else:\n",
    "                whale_id = \"\"\n",
    "            if remove_new_whale and whale_id == \"new_whale\":\n",
    "                continue\n",
    "            \n",
    "            ## Choose if storing in train of dev dataset\n",
    "            if len(glob.glob(\"/\".join([sub_dataset_train_path, whale_id]))):\n",
    "                # If 80% uniform and dev aleady exist\n",
    "                if random.uniform(0, 1) >= train_dev_ratio and len(glob.glob(\"/\".join([sub_dataset_dev_path, whale_id]))):\n",
    "                    # We store in train dataset\n",
    "                    sub_dataset_path = sub_dataset_train_path\n",
    "                else:\n",
    "                    # We store in dev dataset\n",
    "                    sub_dataset_path = sub_dataset_dev_path\n",
    "            else:\n",
    "                sub_dataset_path = sub_dataset_train_path\n",
    "            \n",
    "            # Create folder and generate path name\n",
    "            self.create_folder_is_needed(\"/\".join([sub_dataset_path, whale_id]).replace(\"//\", \"/\"))\n",
    "            orig_path = \"/\".join([self.dataset_path_train, whale_file_name]).replace(\"//\", \"/\")\n",
    "            target_path = \"/\".join([sub_dataset_path, whale_id, whale_file_name]).replace(\"//\", \"/\")\n",
    "            if crop:\n",
    "                cropped = self.get_convex_masked_tail(orig_path)\n",
    "                cv2.imwrite(target_path, cropped)\n",
    "            else:\n",
    "                shutil.copy(orig_path, target_path)\n",
    "                \n",
    "        print(\"Done\" + \" \"*20)\n",
    "        \n",
    "        ## Removing folder that doesn't have a dev equivalent\n",
    "        for train_path in glob.glob(sub_dataset_train_path + \"/*/\"):\n",
    "            in_train_id = train_path.split(\"/\")[-2]\n",
    "            equivalent_dev_id_path = \"/\".join([sub_dataset_dev_path, in_train_id])\n",
    "            if not os.path.isdir(equivalent_dev_id_path):\n",
    "                shutil.rmtree(train_path)\n",
    "    \n",
    "    def disp_image(self, im, gray=False):\n",
    "        \"\"\"\n",
    "        Display an image in a plt plot\n",
    "        :param im: image, numpy array format, used by cv2\n",
    "        \"\"\"\n",
    "        \n",
    "        if gray:\n",
    "            imgplot = plt.imshow(im, cmap='gray')\n",
    "        else:\n",
    "            imgplot = plt.imshow(im)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def dual_threshold(self, im_temp, disp=False):\n",
    "        \n",
    "        # Compute lighter threshold\n",
    "        lighter = np.max(im_temp)\n",
    "        ret, im_temp1 = cv2.threshold(im_temp, lighter*0.9, 255, cv2.THRESH_BINARY)\n",
    "        if disp:\n",
    "            print(\"thres1\")\n",
    "            self.disp_image(im_temp1, gray=True)\n",
    "        \n",
    "        # Compute darker threshold\n",
    "        darker = np.min(im_temp)\n",
    "        ret, im_temp2 = cv2.threshold(im_temp, (darker+10)*1.5, 255, cv2.THRESH_BINARY_INV)\n",
    "        if disp:\n",
    "            print(\"thres2\")\n",
    "            self.disp_image(im_temp2, gray=True)\n",
    "        \n",
    "        # Merging\n",
    "        im_temp = cv2.add(im_temp1, im_temp2)\n",
    "        if disp:\n",
    "            print(\"threshold merge\")    \n",
    "            self.disp_image(im_temp, gray=True)\n",
    "            \n",
    "        im_temp = cv2.GaussianBlur(im_temp, (9, 9), 0)\n",
    "        \n",
    "        return im_temp\n",
    "    \n",
    "    \n",
    "    def otsu(self, im_temp, disp):\n",
    "        \n",
    "        ret, im_temp = cv2.threshold(im_temp,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        if disp:\n",
    "            self.disp_image(im_temp, gray=True)\n",
    "        \n",
    "        return im_temp\n",
    "    \n",
    "    def compute_contour(self, im_temp):\n",
    "\n",
    "        contours, _ = cv2.findContours(im_temp, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        areas = []\n",
    "        for cnt in contours:\n",
    "            areas.append(cv2.contourArea(cnt))\n",
    "        \n",
    "        # get bigger area and store its contour\n",
    "        max_area = areas[np.argmax(areas)]\n",
    "        cnt = contours[np.argmax(areas)]\n",
    "        return max_area, cnt\n",
    "        \n",
    "    \n",
    "    def get_convex_masked_tail(self, im_path, disp=False):\n",
    "        \"\"\"\n",
    "        Return the image of the tail-convex-crop\n",
    "        :param im_path: str, path to the image\n",
    "        :param disp: bool, set to true to display\n",
    "        \"\"\"\n",
    "        # Load image and resize witout changing ratio\n",
    "        im = cv2.imread(im_path)\n",
    "        if disp:\n",
    "            print(\"Original\")\n",
    "            self.disp_image(im)\n",
    "        \n",
    "        height, width = im.shape[:2]\n",
    "        new_width = 200\n",
    "        new_height = new_width*height//width\n",
    "        im = cv2.resize(im, (new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # Change to gray and apply both gaussian and threshold filter\n",
    "        im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        im_temp = im_gray.copy()\n",
    "        \n",
    "        # default preprocessing\n",
    "        dual_threshold = False\n",
    "        otsu = True\n",
    "            \n",
    "        if otsu:\n",
    "            im_temp = self.otsu(im_temp, disp)\n",
    "            max_area, cnt = self.compute_contour(im_temp)\n",
    "            if max_area <  1000:\n",
    "                dual_threshold = False\n",
    "                otsu = False\n",
    "        \n",
    "        if dual_threshold:\n",
    "            im_temp = self.dual_threshold(im_temp, disp)\n",
    "            max_area, cnt = self.compute_contour(im_temp)\n",
    "            if max_area < 1000:\n",
    "                dual_threshold = False\n",
    "        \n",
    "        # Check if used preprocessing cropping\n",
    "        if otsu or dual_threshold:\n",
    "            # Get convex contour\n",
    "            cnt_hull = cv2.convexHull(cnt)\n",
    "            \n",
    "            crop = np.zeros_like(im)\n",
    "            for x in range(crop.shape[1]):\n",
    "                for y in range(crop.shape[0]):\n",
    "                    if cv2.pointPolygonTest(cnt_hull, (x, y), False) == 1:\n",
    "                        crop[y, x] = im_gray[y, x]\n",
    "\n",
    "            if disp:\n",
    "                print(\"im crop\")\n",
    "                print(np.max(areas))\n",
    "                self.disp_image(crop, gray=True)\n",
    "\n",
    "                print(\"raw contour\")\n",
    "                im2 = cv2.drawContours(im.copy(), cnt, -1, (255,0,0), 4)\n",
    "                self.disp_image(im2)\n",
    "\n",
    "                print(\"convex contour\")\n",
    "                im3 = cv2.drawContours(im.copy(), cnt_hull, -1, (255,0,0), 10)\n",
    "                self.disp_image(im3)\n",
    "        else:\n",
    "            # If preprocessing not relevant, keep gray image\n",
    "            crop = im_gray\n",
    "        \n",
    "        return crop\n",
    "\n",
    "# Create the dataset object\n",
    "dataset = Dataset(\"dataset\")\n",
    "# Split the dataset into the train_classes folder\n",
    "dataset.split_in_classes_folders(\"dataset/train_classes\", remove_new_whale=False, remove_old=True, train_dev_ratio=0.2, crop=True, split=False)\n",
    "\n",
    "# a = dataset.get_convex_masked_tail(\"dataset/train/5422b5568.jpg\", True)\n",
    "# a = dataset.get_convex_masked_tail(\"dataset/train/0a0c1df99.jpg\", True)\n",
    "# a = dataset.get_convex_masked_tail(\"dataset/train/17f3cae5a.jpg\", True)\n",
    "# a = dataset.get_convex_masked_tail(\"dataset/train/00de0f4c8.jpg\", True)\n",
    "# \"dataset/train/0a0c1df99.jpg\"\n",
    "# \"dataset/train/0a1a0c3f7.jpg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
