{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whales identification from their tails - Kaggle contest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset/train.csv\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Manage dataset loading\n",
    "    \n",
    "    :param dataset_path: str, path to the dataset folder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        \n",
    "        # Build and store dataset paths\n",
    "        self.dataset_path_train = dataset_path + \"/train\"\n",
    "        self.dataset_path_test = dataset_path + \"/test\"\n",
    "        \n",
    "        # Build train.csv path\n",
    "        self.dataset_path_train_label = dataset_path + \"/train.csv\"\n",
    "        \n",
    "        # Generate pandas dataframe of whales id <-> file matching\n",
    "        self.dataset_train_label = self.get_train_label()\n",
    "        \n",
    "        ## Get pictures paths\n",
    "#         self.les_im_path_train = self.get_im_path(self.dataset_path_train + \"/*.jpg\")\n",
    "#         self.les_im_path_test = self.get_im_path(self.dataset_path_test + \"/*.jpg\")\n",
    "        \n",
    "    def get_im_path(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Get pictures path under the given folder path\n",
    "        :param dataset_path: str, path to the dataset folder\n",
    "        \n",
    "        :output les_im_path: list of string, .jpg picture paths under the dataset_path folder\n",
    "        \"\"\"\n",
    "        print(\"Getting images path from\", dataset_path)\n",
    "        les_im_path = glob.glob(dataset_path)\n",
    "        les_im_path.sort()\n",
    "        return les_im_path\n",
    "    \n",
    "    def get_train_label(self):\n",
    "        \"\"\"\n",
    "        Load the train dataset annotation using pandas\n",
    "        \n",
    "        :return train_label: pandas dataframe, whales id <-> files matching\n",
    "        \"\"\"\n",
    "        print(\"Loading\", self.dataset_path_train_label)\n",
    "        return pd.read_csv(self.dataset_path_train_label)\n",
    "    \n",
    "    \n",
    "    def create_folder_is_needed(self, folder_path):\n",
    "        \"\"\"\n",
    "        Create a folder if it doesn't alreadt exist\n",
    "        :param folder_path: str\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "    \n",
    "    def split_in_classes_folders(self, root_classes_folder, pass_new_whale=True, remove_old=True, train_dev_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Split the dataset into classes folders\n",
    "        :param root_classes_folder:\n",
    "        :param pass_new_whale: boolean, set to true if not considering new_whale id\n",
    "        \n",
    "        OUTPUT:\n",
    "            pictures sort by whale id into subfolders of the root_classes_folder\n",
    "        \"\"\"\n",
    "        \n",
    "        if remove_old:\n",
    "            print(\"Removing previous spliting\")\n",
    "            shutil.rmtree(root_classes_folder)\n",
    "        \n",
    "        sub_dataset_train_path = \"/\".join([root_classes_folder, \"train\"])\n",
    "        sub_dataset_dev_path = \"/\".join([root_classes_folder, \"dev\"])\n",
    "        \n",
    "        # Create the folders if needed\n",
    "        self.create_folder_is_needed(root_classes_folder)\n",
    "        self.create_folder_is_needed(sub_dataset_train_path)\n",
    "        self.create_folder_is_needed(sub_dataset_dev_path)\n",
    "        \n",
    "        # If passing new whales, should remove previous folder\n",
    "        new_whale_folder_path = \"/\".join([root_classes_folder, \"new_whale\"])\n",
    "        if pass_new_whale and os.path.isdir(new_whale_folder_path):\n",
    "            print(\"Removing new_whale folder\")\n",
    "            shutil.rmtree(new_whale_folder_path)\n",
    "        \n",
    "        # Sorting the images\n",
    "        files_number = len(self.dataset_train_label)\n",
    "        print(\"Sorting\", files_number, \"images into\", root_classes_folder)\n",
    "        \n",
    "        for index, row in self.dataset_train_label.iterrows():\n",
    "            print(\"#\" + str(index + 1) + \"/\" + str(files_number), end=\"\\r\")\n",
    "            whale_file_name = row['Image']\n",
    "            whale_id = row['Id']\n",
    "            if pass_new_whale and whale_id == \"new_whale\":\n",
    "                continue\n",
    "            \n",
    "            ## Choose if storing in train of dev dataset\n",
    "            if random.uniform(0, 1) >= 0.2:\n",
    "                # We store in train dataset\n",
    "                sub_dataset_path = sub_dataset_train_path\n",
    "            else:\n",
    "                # We store in dev dataset\n",
    "                sub_dataset_path = sub_dataset_dev_path\n",
    "            \n",
    "            self.create_folder_is_needed(\"/\".join([sub_dataset_path, whale_id]))\n",
    "            shutil.copy(\n",
    "                \"/\".join([self.dataset_path_train, whale_file_name]), \n",
    "                \"/\".join([sub_dataset_path, whale_id, whale_file_name])\n",
    "            )\n",
    "        print(\"Done\" + \" \"*20)\n",
    "            \n",
    "            \n",
    "# Create the dataset object\n",
    "dataset = Dataset(\"dataset\")\n",
    "# Split the dataset into the train_classes folder\n",
    "# dataset.split_in_classes_folders(\"dataset/train_classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (150, 150, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3813)              70529061  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3813)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3813)              0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3813)              0         \n",
      "=================================================================\n",
      "Total params: 70,557,701\n",
      "Trainable params: 70,557,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 7280 images belonging to 3813 classes.\n",
      "Found 1760 images belonging to 1372 classes.\n",
      "Epoch 1/50\n",
      " 37/910 [>.............................] - ETA: 11:20 - loss: 8.2472 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, dataset_path):\n",
    "        self.img_width = 150\n",
    "        self.img_height = 150\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_path_train = \"/\".join([dataset_path, \"train\"])\n",
    "        self.dataset_path_dev = \"/\".join([dataset_path, \"dev\"])\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 8\n",
    "        self.len_train = len(glob.glob(self.dataset_path_train + \"/*/*.jpg\"))\n",
    "        self.len_dev = len(glob.glob(self.dataset_path_dev + \"/*/*.jpg\"))\n",
    "        self.n_train_label = len(glob.glob(self.dataset_path_train + \"/*/\"))\n",
    "    \n",
    "    def run_training(self):\n",
    "        \n",
    "        ## Build model architecture\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_shape = (3, self.img_width, self.img_height)\n",
    "        else:\n",
    "            input_shape = (self.img_width, self.img_height, 3)\n",
    "        \n",
    "        print(\"input_shape\", input_shape)\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(32, (3, 3)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Conv2D(64, (3, 3)))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(self.n_train_label))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Activation('softmax'))\n",
    "\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        # plot_model(self.model, to_file='model.png')\n",
    "        self.model.summary()\n",
    "\n",
    "        ## Define data generation\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            self.dataset_path_train,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        validation_generator = test_datagen.flow_from_directory(\n",
    "            self.dataset_path_dev,\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical')\n",
    "    \n",
    "        \n",
    "        self.model.fit_generator(\n",
    "            train_generator,\n",
    "            epochs=self.epochs,\n",
    "            steps_per_epoch=self.len_train//self.batch_size,\n",
    "            validation_data=validation_generator, \n",
    "            validation_steps=self.len_dev//self.batch_size)\n",
    "        \n",
    "\n",
    "        self.model.save_weights('first_try.h5')\n",
    "\n",
    "modeler = Model(\"dataset/train_classes\")\n",
    "modeler.run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
